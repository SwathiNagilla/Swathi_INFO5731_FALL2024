{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwathiNagilla/Swathi_INFO5731_FALL2024/blob/main/INFO5731_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "#Function to Extract movie reviews from IMDB\n",
        "def get_reviews(movie_url, max_reviews=1000):\n",
        "    reviews = []\n",
        "    page = 1\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "      #set header\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        url = f\"{movie_url}reviews/_ajax?ref_=undefined&paginationKey=page={page}\"\n",
        "        #Make a GET request to fetch the reviews\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f'Failed to retrieve reviews for page {page}. Status code: {response.status_code}')\n",
        "            break\n",
        "        #Parse the response content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract reviews\n",
        "        review_elements = soup.find_all('div', class_='review-container')\n",
        "        if not review_elements:\n",
        "            print(\" \")\n",
        "            break\n",
        "\n",
        "        for review_element in review_elements:\n",
        "            try:\n",
        "              #Extract the text of the review\n",
        "                review_text = review_element.find('div', class_='text').get_text(strip=True)\n",
        "                reviews.append(review_text)\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "        print(f'Collected {len(reviews)} reviews from page {page}.')\n",
        "        page += 1\n",
        "    #Return the list of collected reviews\n",
        "    return reviews\n",
        "#Function to save reviews to CSV file\n",
        "def save_reviews_to_csv(reviews, filename='movie_reviews.csv'):\n",
        "    df = pd.DataFrame(reviews, columns=['Review'])\n",
        "    df.to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f'Saved {len(reviews)} reviews to {filename}.')\n",
        "\n",
        "# IMDb movie URL for reviews\n",
        "movie_url = 'https://www.imdb.com/title/tt1431045/reviews/#'\n",
        "all_reviews = get_reviews(movie_url)\n",
        "save_reviews_to_csv(all_reviews)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9GPUEm3fsdb",
        "outputId": "c5c5817d-9abb-4955-aadd-96515c8d8008"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 25 reviews from page 1.\n",
            "Collected 50 reviews from page 2.\n",
            "Collected 75 reviews from page 3.\n",
            "Collected 100 reviews from page 4.\n",
            "Collected 125 reviews from page 5.\n",
            "Collected 150 reviews from page 6.\n",
            "Collected 175 reviews from page 7.\n",
            "Collected 200 reviews from page 8.\n",
            "Collected 225 reviews from page 9.\n",
            "Collected 250 reviews from page 10.\n",
            "Collected 275 reviews from page 11.\n",
            " \n",
            "Saved 275 reviews to movie_reviews.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea2d5cc-c291-47c1-f229-725e19108242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned reviews to cleaned_movie_reviews.csv.\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the reviews\n",
        "data = pd.read_csv('movie_reviews.csv')\n",
        "\n",
        "def clean(text):\n",
        "    # 1. Remove noise, special characters, and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 2. Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "    # 4. Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # 5. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    text = ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "    # 6. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Clean the reviews and save to a new column\n",
        "data['Cleaned_Review'] = data['Review'].apply(clean)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "data.to_csv('cleaned_movie_reviews.csv', index=False, encoding='utf-8')\n",
        "print('Saved cleaned reviews to cleaned_movie_reviews.csv.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk, RegexpParser\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Xokl4yQ1IPx",
        "outputId": "8c114b10-a6b3-4e6f-8164-d7bd2e09d199"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read the file\n",
        "data= pd.read_csv('cleaned_movie_reviews.csv')\n",
        "\n",
        "# Function to perform POS tagging and count occurrences\n",
        "def pos(text):\n",
        "    token = word_tokenize(text)\n",
        "    pos_tags = pos_tag(token)\n",
        "    return pos_tags\n",
        "\n",
        "# Seperate POS tags for all reviews\n",
        "all_tags = []\n",
        "for review in df['Cleaned_Review']:\n",
        "    all_tags.extend(pos(review))\n",
        "\n",
        "# Count occurrences of each POS tag\n",
        "pos_counts = Counter(tag for word, tag in all_tags)\n",
        "nouns = pos_counts['NN'] + pos_counts['NNS'] + pos_counts['NNP'] + pos_counts['NNPS']\n",
        "verbs = pos_counts['VB'] + pos_counts['VBD'] + pos_counts['VBG'] + pos_counts['VBN'] + pos_counts['VBP'] + pos_counts['VBZ']\n",
        "adjectives = pos_counts['JJ'] + pos_counts['JJR'] + pos_counts['JJS']\n",
        "adverbs = pos_counts['RB'] + pos_counts['RBR'] + pos_counts['RBS']\n",
        "\n",
        "print(f'Total Nouns: {nouns}')\n",
        "print(f'Total Verbs: {verbs}')\n",
        "print(f'Total Adjectives: {adjectives}')\n",
        "print(f'Total Adverbs: {adverbs}')\n",
        "\n",
        "# Function for constituency parsing\n",
        "def constituency_parsing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    grammar = r\"\"\"\n",
        "      NP: {<DT>?<JJ>*<NN.*>}   # Noun Phrase\n",
        "      VP: {<VB.*><NP|PP>*}      # Verb Phrase\n",
        "      \"\"\"\n",
        "    cp = RegexpParser(grammar)\n",
        "    tree = cp.parse(pos_tags)\n",
        "    return tree\n",
        "\n",
        "# Take review from the dataset\n",
        "sentence = data['Cleaned_Review'].iloc[0]\n",
        "constituency_tree = constituency_parsing(sentence)\n",
        "print(\"\\nConstituency Parsing Tree:\")\n",
        "print(constituency_tree)\n",
        "\n",
        "# Function for dependency parsing using spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "dependency_tree = dependency_parsing(sentence)\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "for token in dependency_tree:\n",
        "    print(token)\n",
        "\n",
        "# Function for Named Entity Recognition\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Aggregate NER results\n",
        "all_entities = []\n",
        "for review in data['Cleaned_Review']:\n",
        "    all_entities.extend(named_entity_recognition(review))\n",
        "\n",
        "# Count entities\n",
        "entity_counts = Counter(entity for entity, label in all_entities)\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "for entity, count in entity_counts.items():\n",
        "    print(f\"{entity}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjCHqEua1GNT",
        "outputId": "93f71697-cf7b-43f4-e1b5-f011864bb3df"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Nouns: 17616\n",
            "Total Verbs: 3840\n",
            "Total Adjectives: 6192\n",
            "Total Adverbs: 1408\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "(S\n",
            "  (NP deadpool/NN)\n",
            "  (NP absolut/NN)\n",
            "  (NP hilari/NN)\n",
            "  (NP stori/NN)\n",
            "  (NP isnt/JJ groundbreak/NN)\n",
            "  (NP trick/NN)\n",
            "  (NP goe/NN)\n",
            "  (NP action/NN)\n",
            "  (NP isnt/NN)\n",
            "  bad/JJ\n",
            "  either/RB\n",
            "  (NP great/JJ film/NN)\n",
            "  (NP lie/NN)\n",
            "  (NP humor/NN)\n",
            "  (NP uniqu/JJ isnt/NN)\n",
            "  like/IN\n",
            "  (NP everi/NN)\n",
            "  (NP superhero/NN)\n",
            "  (NP film/NN)\n",
            "  whether/IN\n",
            "  (NP talk/NN)\n",
            "  (NP constant/JJ break/NN)\n",
            "  (NP fourth/JJ wall/NN)\n",
            "  (NP rrate/NN)\n",
            "  (VP\n",
            "    differ/VBP\n",
            "    (NP characterist/NN)\n",
            "    (NP film/NN)\n",
            "    (NP differenti/NN)\n",
            "    (NP superhero/NN)\n",
            "    (NP movi/NN))\n",
            "  (VP frank/VBD (NP arguabl/JJ doesnt/NN))\n",
            "  even/RB\n",
            "  (VP\n",
            "    justifi/VBZ\n",
            "    (NP superhero/NN)\n",
            "    (NP stamp/NN)\n",
            "    (NP someth/NN)\n",
            "    (NP differentit/NN)\n",
            "    (NP need/NN))\n",
            "  (VP said/VBD (NP film/NN))\n",
            "  might/MD\n",
            "  (VP\n",
            "    everyon/VB\n",
            "    (NP crude/NN)\n",
            "    (NP fill/NN)\n",
            "    (NP nuditi/JJ violenc/NN)\n",
            "    (NP humor/NN)\n",
            "    (NP wont/JJ appeal/NN)\n",
            "    (NP everyon/NN))\n",
            "  either/CC\n",
            "  (NP quit/NN)\n",
            "  (NP dark/NN)\n",
            "  (NP sometim/NN)\n",
            "  (NP satir/JJ sure/JJ deadpool/NN)\n",
            "  (NP isnt/NN)\n",
            "  (NP everyon/NN)\n",
            "  right/RB\n",
            "  audienc/RB\n",
            "  (NP masterpiec/JJ humor/NN)\n",
            "  (NP great/JJ countless/NN)\n",
            "  (NP hilari/NN)\n",
            "  (NP joke/NN)\n",
            "  throughout/IN\n",
            "  (VP\n",
            "    make/VBP\n",
            "    (NP laugh/JJ ryan/NN)\n",
            "    (NP reynold/NN)\n",
            "    (NP perfect/JJ deadpool/NN))\n",
            "  well/RB\n",
            "  (NP role/NN)\n",
            "  (NP truli/NNS)\n",
            "  (NP great/JJ perform/JJ himto/NN)\n",
            "  (NP sum/NN)\n",
            "  (NP deadpool/NN)\n",
            "  (NP great/JJ film/NN)\n",
            "  might/MD\n",
            "  (VP everyon/VB)\n",
            "  (VP recommend/VB))\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "('deadpool', 'compound', 'absolut')\n",
            "('absolut', 'compound', 'stori')\n",
            "('hilari', 'compound', 'stori')\n",
            "('stori', 'nsubj', 'is')\n",
            "('is', 'ccomp', 'said')\n",
            "('nt', 'neg', 'is')\n",
            "('groundbreak', 'amod', 'action')\n",
            "('trick', 'compound', 'goe')\n",
            "('goe', 'compound', 'action')\n",
            "('action', 'nsubj', 'is')\n",
            "('is', 'ccomp', 'is')\n",
            "('nt', 'neg', 'is')\n",
            "('bad', 'acomp', 'is')\n",
            "('either', 'advmod', 'great')\n",
            "('great', 'amod', 'film')\n",
            "('film', 'compound', 'humor')\n",
            "('lie', 'compound', 'humor')\n",
            "('humor', 'compound', 'uniqu')\n",
            "('uniqu', 'nsubj', 'is')\n",
            "('is', 'ccomp', 'is')\n",
            "('nt', 'neg', 'is')\n",
            "('like', 'prep', 'is')\n",
            "('everi', 'amod', 'film')\n",
            "('superhero', 'compound', 'film')\n",
            "('film', 'pobj', 'like')\n",
            "('whether', 'mark', 'talk')\n",
            "('talk', 'nsubj', 'differ')\n",
            "('constant', 'amod', 'break')\n",
            "('break', 'compound', 'rrate')\n",
            "('fourth', 'prt', 'break')\n",
            "('wall', 'compound', 'rrate')\n",
            "('rrate', 'nsubj', 'differ')\n",
            "('differ', 'ccomp', 'is')\n",
            "('characterist', 'amod', 'movi')\n",
            "('film', 'compound', 'movi')\n",
            "('differenti', 'compound', 'superhero')\n",
            "('superhero', 'compound', 'movi')\n",
            "('movi', 'dobj', 'differ')\n",
            "('frank', 'compound', 'arguabl')\n",
            "('arguabl', 'nsubj', 'said')\n",
            "('does', 'aux', 'need')\n",
            "('nt', 'neg', 'need')\n",
            "('even', 'advmod', 'stamp')\n",
            "('justifi', 'compound', 'superhero')\n",
            "('superhero', 'nmod', 'stamp')\n",
            "('stamp', 'nmod', 'need')\n",
            "('someth', 'amod', 'differentit')\n",
            "('differentit', 'compound', 'need')\n",
            "('need', 'nsubj', 'said')\n",
            "('said', 'ROOT', 'said')\n",
            "('film', 'nsubj', 'everyon')\n",
            "('might', 'aux', 'everyon')\n",
            "('everyon', 'ccomp', 'said')\n",
            "('crude', 'amod', 'fill')\n",
            "('fill', 'compound', 'nuditi')\n",
            "('nuditi', 'compound', 'humor')\n",
            "('violenc', 'compound', 'humor')\n",
            "('humor', 'nsubj', 'appeal')\n",
            "('wo', 'aux', 'appeal')\n",
            "('nt', 'neg', 'appeal')\n",
            "('appeal', 'ccomp', 'everyon')\n",
            "('everyon', 'dobj', 'appeal')\n",
            "('either', 'preconj', 'quit')\n",
            "('quit', 'conj', 'appeal')\n",
            "('dark', 'amod', 'satir')\n",
            "('sometim', 'compound', 'satir')\n",
            "('satir', 'dobj', 'quit')\n",
            "('sure', 'advmod', 'everyon')\n",
            "('deadpool', 'nsubj', 'is')\n",
            "('is', 'auxpass', 'everyon')\n",
            "('nt', 'neg', 'everyon')\n",
            "('everyon', 'advcl', 'said')\n",
            "('right', 'amod', 'audienc')\n",
            "('audienc', 'npadvmod', 'everyon')\n",
            "('masterpiec', 'nsubj', 'humor')\n",
            "('humor', 'ccomp', 'everyon')\n",
            "('great', 'amod', 'joke')\n",
            "('countless', 'amod', 'joke')\n",
            "('hilari', 'compound', 'joke')\n",
            "('joke', 'dobj', 'humor')\n",
            "('throughout', 'prep', 'humor')\n",
            "('make', 'pcomp', 'throughout')\n",
            "('laugh', 'ccomp', 'make')\n",
            "('ryan', 'amod', 'reynold')\n",
            "('reynold', 'nmod', 'deadpool')\n",
            "('perfect', 'amod', 'deadpool')\n",
            "('deadpool', 'dobj', 'make')\n",
            "('well', 'amod', 'role')\n",
            "('role', 'nsubj', 'everyon')\n",
            "('truli', 'nmod', 'film')\n",
            "('great', 'amod', 'sum')\n",
            "('perform', 'compound', 'sum')\n",
            "('himto', 'compound', 'sum')\n",
            "('sum', 'compound', 'film')\n",
            "('deadpool', 'nmod', 'film')\n",
            "('great', 'amod', 'film')\n",
            "('film', 'nsubj', 'everyon')\n",
            "('might', 'aux', 'everyon')\n",
            "('everyon', 'ROOT', 'everyon')\n",
            "('recommend', 'xcomp', 'everyon')\n",
            "\n",
            "Named Entity Counts:\n",
            "hilari: 88\n",
            "everi superhero: 22\n",
            "fourth: 77\n",
            "justifi superhero: 11\n",
            "one: 209\n",
            "merc: 55\n",
            "wolverin movi brilliantli: 11\n",
            "notchim happi movi rate water movi honestli: 11\n",
            "realli enjoy movi: 11\n",
            "funni: 33\n",
            "realli realli: 11\n",
            "superhero movi prais: 11\n",
            "fulli: 11\n",
            "first: 132\n",
            "first moment liter: 11\n",
            "insan oddli: 11\n",
            "everi camera techniqu avail: 11\n",
            "mani movi lackth: 11\n",
            "everi line: 11\n",
            "everi joke: 11\n",
            "reveng: 33\n",
            "mormon: 11\n",
            "violenc superhero: 11\n",
            "genr: 11\n",
            "believ comic book pictur film adapt: 11\n",
            "anyon el: 11\n",
            "robert downey jr iron: 11\n",
            "narr: 11\n",
            "narrativeon trademark: 11\n",
            "today: 22\n",
            "tim miller: 44\n",
            "cinematograph ken seng: 11\n",
            "ajax honestli servic ed skrein: 11\n",
            "dick british: 11\n",
            "ann murray: 11\n",
            "interrupt colossu: 11\n",
            "kapic introduc: 11\n",
            "teen negason: 11\n",
            "green lantern: 11\n",
            "wade wilson becom: 11\n",
            "zero: 11\n",
            "onlin: 11\n",
            "year year: 11\n",
            "plebeian: 11\n",
            "million: 11\n",
            "weekend month: 11\n",
            "februari surpass: 11\n",
            "grey biggest: 11\n",
            "weekend: 22\n",
            "centuri fox surpass: 11\n",
            "tim miller directori: 11\n",
            "past year: 11\n",
            "fox: 11\n",
            "hilari intro: 11\n",
            "everi singl: 33\n",
            "second: 11\n",
            "year one: 11\n",
            "genr deadpool: 11\n",
            "explosionsmi primari: 11\n",
            "superhero: 11\n",
            "ton ton: 11\n",
            "dark knight dark knight rise: 11\n",
            "antihero known constantli: 11\n",
            "awar mani: 11\n",
            "perceiv comedian ryan reynold: 11\n",
            "believ mani marvel movi: 11\n",
            "worri polit: 11\n",
            "mani movi: 11\n",
            "nowaday: 11\n",
            "abus: 11\n",
            "assur: 11\n",
            "year old: 11\n",
            "nuditi movi sen: 11\n",
            "straight hour: 11\n",
            "numpti: 11\n",
            "earth: 11\n",
            "dark knight: 11\n",
            "xmeni: 11\n",
            "chum: 11\n",
            "cheeki onlin: 11\n",
            "richard kettl: 11\n",
            "natur: 11\n",
            "nonsenseth pace movi: 11\n",
            "season: 11\n",
            "eighth: 11\n",
            "waywad wilson mercenari: 11\n",
            "new york: 11\n",
            "nearli year: 11\n",
            "one night: 11\n",
            "night: 11\n",
            "vanessa middl night: 11\n",
            "tortur day: 11\n",
            "trigger mutat week: 11\n",
            "al advic: 11\n",
            "colossu negason: 22\n",
            "colossu handcuff: 11\n",
            "next night: 11\n",
            "scrapyarddeadpool: 11\n",
            "colossu: 11\n",
            "negason gruel battl angel: 11\n",
            "armi: 11\n",
            "colossu urg: 11\n",
            "materi: 11\n",
            "david foster: 11\n",
            "don mask becom: 11\n",
            "ripoff sam raimi darkman: 11\n",
            "miller: 33\n",
            "dedic sexi white: 11\n",
            "masculin constantli: 11\n",
            "insecur convent action: 11\n",
            "tri assert tough: 11\n",
            "wilson sexi invinc: 11\n",
            "wham prospect fondl ball: 11\n",
            "everyth deadpool postmodern whole: 11\n",
            "masculin: 11\n",
            "nike: 11\n",
            "six: 11\n",
            "sixth: 11\n",
            "year: 22\n",
            "centuri fox privat: 11\n",
            "paul wernick: 11\n",
            "kick wade: 11\n",
            "wilson deadpool taxi mission: 11\n",
            "tom holkenborg: 11\n",
            "junki xl: 11\n",
            "chao: 11\n",
            "everyon: 11\n",
            "terrif: 11\n",
            "julian clark handl: 11\n",
            "stan lee cameo: 11\n",
            "believ: 11\n",
            "plea mani fan wish day: 11\n",
            "genr heighten: 11\n",
            "materi embrac: 11\n",
            "multipl facet: 11\n",
            "substanc depth: 11\n",
            "amus empti: 11\n",
            "negason: 11\n",
            "el note humor: 11\n",
            "himryan: 11\n",
            "prais: 11\n",
            "year ago: 11\n",
            "ten: 11\n",
            "extrem: 11\n",
            "funni awesom film: 11\n",
            "seri: 11\n",
            "one film year old: 11\n",
            "groin: 11\n",
            "funni stop funni: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://myunt-my.sharepoint.com/:x:/g/personal/swathinagilla_my_unt_edu/Eb91etIja8VMnXWOwuWhNTQBSDMxyqnMB_gym2tZE_kNHg?e=saTiKg"
      ],
      "metadata": {
        "id": "IkSRHr4b_Xm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The assignment was a good learning experience for web scraping and data analysis. I struggled a bit with pulling reviews from IMDb\n",
        "because the website's structure kept changing and I found it hard to manage my time on\n",
        "all the tasks. Still, I enjoyed learning about scraping, seeing my code work, and figuring out how to clean the data. The time given\n",
        "felt tight but it pushed me to work efficiently.'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}