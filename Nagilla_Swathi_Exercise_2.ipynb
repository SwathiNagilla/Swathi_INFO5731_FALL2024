{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwathiNagilla/Swathi_INFO5731_FALL2024/blob/main/Nagilla_Swathi_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Research Question: How have user reviews for popular tech products changed in the past year.\n",
        "\n",
        "Data Required:Product Name\n",
        "              Review Text\n",
        "              Sentiment (positive/negative)\n",
        "              Review Date\n",
        "\n",
        "For data collection, I'll be collecting reviews of 10 top-rated technical products in the last year.\n",
        "Each product will have at least 100 user reviews, adding up to approximately 1,000 in total. I will\n",
        "use web scraping tools to collect this data and try to focus on getting review text and usernames.\n",
        "I'll then check, after gathering the reviews, whether each review is positive or negative using TextBlob or VADER.\n",
        "Then, sorted data of product name, review text, sentiment score, and username are put into a CSV file for more analysis.\n",
        "\n",
        "Data Collection Steps:\n",
        "- Identify one source of tech product reviews.\n",
        "- Extract reviews for products using Python and BeautifulSoup.\n",
        "- Do a sentiment analysis of each review using TextBlob.\n",
        "- Save the data into a CSV file.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generate random reviews\n",
        "def random_reviews(num_reviews):\n",
        "    products = [\"Laptop\", \"Smartphone\", \"Headphones\", \"Smartwatch\", \"Tablet\"]\n",
        "    sentiments = [\"Positive\", \"Negative\"]\n",
        "\n",
        "    reviews = []\n",
        "    for _ in range(num_reviews):\n",
        "        #use random function to generate random product from products list\n",
        "        product = random.choice(products)\n",
        "        review_text = f\"This is a {'good' if random.choice(sentiments) == 'Positive' else 'bad'} review for {product}.\"\n",
        "        #use random function to generate random sentiment review\n",
        "        sentiment = random.choice(sentiments)\n",
        "        review_date = datetime.now() - timedelta(days=random.randint(0, 365))\n",
        "\n",
        "        reviews.append({\n",
        "            \"Product Name\": product,\n",
        "            \"Review Text\": review_text,\n",
        "            \"Sentiment\": sentiment,\n",
        "            \"Review Date\": review_date.strftime(\"%Y-%m-%d\")\n",
        "        })\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Save data\n",
        "num_reviews = 1000\n",
        "data = random_reviews(num_reviews)\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('tech_product_reviews.csv', index=False)\n",
        "\n",
        "# Print the data containing the first 15 rows\n",
        "print(df.head(15))\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb55550a-187a-495a-d5a5-96c225e25fd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Product Name                            Review Text Sentiment Review Date\n",
            "0        Laptop      This is a good review for Laptop.  Positive  2024-06-17\n",
            "1        Laptop       This is a bad review for Laptop.  Positive  2023-11-03\n",
            "2        Tablet      This is a good review for Tablet.  Negative  2024-07-19\n",
            "3        Laptop      This is a good review for Laptop.  Negative  2023-12-01\n",
            "4    Smartwatch   This is a bad review for Smartwatch.  Positive  2024-07-03\n",
            "5    Smartwatch   This is a bad review for Smartwatch.  Positive  2023-11-11\n",
            "6        Laptop      This is a good review for Laptop.  Positive  2024-04-13\n",
            "7    Smartwatch   This is a bad review for Smartwatch.  Positive  2023-11-15\n",
            "8    Smartwatch  This is a good review for Smartwatch.  Positive  2024-06-14\n",
            "9    Headphones   This is a bad review for Headphones.  Positive  2024-02-07\n",
            "10   Smartwatch  This is a good review for Smartwatch.  Negative  2023-12-04\n",
            "11   Smartwatch   This is a bad review for Smartwatch.  Positive  2024-05-07\n",
            "12       Tablet       This is a bad review for Tablet.  Negative  2024-09-07\n",
            "13   Smartwatch  This is a good review for Smartwatch.  Negative  2023-12-09\n",
            "14       Laptop      This is a good review for Laptop.  Positive  2024-04-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def google_scholar(query, num_articles):\n",
        "    #url with keyword XYZ from gooogle scholar\n",
        "    url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&q=XYZ&btnG=\"\n",
        "    parameter = {\n",
        "        'hl': 'en',\n",
        "        'q': query,\n",
        "        'as_ylo': '2014',\n",
        "        'as_yhi': '2024'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=parameter)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for entry in soup.find_all('div', class_='gs_ri')[:num_articles]:\n",
        "        title_tag = entry.find('h3', class_='gs_rt')\n",
        "        authors_tag = entry.find('div', class_='gs_a')\n",
        "        abstract_tag = entry.find('div', class_='gs_rs')\n",
        "        #extract title and if nothing is present, return N/A\n",
        "        title = title_tag.text if title_tag else \"N/A\"\n",
        "\n",
        "        # extract year\n",
        "        if authors_tag:\n",
        "            parts = authors_tag.text.split()\n",
        "            #identifying digit\n",
        "            year = next((part for part in parts if part.isdigit() and len(part) == 4), \"N/A\")\n",
        "            authors = ', '.join(part.strip() for part in authors_tag.text.split('-')[0].split(', ')) if authors_tag else \"N/A\"\n",
        "        else:\n",
        "            year = \"N/A\"\n",
        "            authors = \"N/A\"\n",
        "        # extract abstract and if nothing is present, return N/A\n",
        "        abstract = abstract_tag.text if abstract_tag else \"N/A\"\n",
        "        #append everything\n",
        "        output.append({'Title': title, 'Authors': authors, 'Year': year, 'Abstract': abstract})\n",
        "    #return output list\n",
        "    return output\n",
        "\n",
        "query = \"natural language processing\"\n",
        "output = google_scholar(query, 1000)\n",
        "\n",
        "data = pd.DataFrame(output)\n",
        "data = data[['Title', 'Authors', 'Year', 'Abstract']]\n",
        "\n",
        "# Save to CSV\n",
        "data.to_csv('google_scholar_articles.csv', index=False)\n",
        "print(data.head(15))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URvaSvPAjUj3",
        "outputId": "a1df6b0f-3de1-48ae-cc60-688910ad204b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0                        Natural language processing   \n",
            "1  [HTML][HTML] Natural language processing: stat...   \n",
            "2            Advances in natural language processing   \n",
            "3  A primer on neural network models for natural ...   \n",
            "4  [PDF][PDF] The Stanford CoreNLP natural langua...   \n",
            "5  Transformers: State-of-the-art natural languag...   \n",
            "6  [BOOK][B] Neural network methods in natural la...   \n",
            "7  Allennlp: A deep semantic natural language pro...   \n",
            "8  Stanza: A Python natural language processing t...   \n",
            "9  Jumping NLP curves: A review of natural langua...   \n",
            "\n",
            "                                    Authors  Year  \\\n",
            "0                KR Chowdhary, KR Chowdhary  2020   \n",
            "1     D Khurana, A Koli, K Khatter, S Singh  2023   \n",
            "2                  J Hirschberg, CD Manning  2015   \n",
            "3                                Y Goldberg  2016   \n",
            "4          CD Manning, M Surdeanu, J Bauer…  2014   \n",
            "5      T Wolf, L Debut, V Sanh, J Chaumond…  2020   \n",
            "6                                Y Goldberg  2017   \n",
            "7  M Gardner, J Grus, M Neumann, O Tafjord…  2018   \n",
            "8         P Qi, Y Zhang, Y Zhang, J Bolton…  2020   \n",
            "9                        E Cambria, B White  2014   \n",
            "\n",
            "                                            Abstract  \n",
            "0  … Python is also available with an extensive s...  \n",
            "1  … in machine specific language, Natural Langua...  \n",
            "2  … Natural language processing employs computat...  \n",
            "3  … More recently, neural network models started...  \n",
            "4  We describe the design and use of the Stanford...  \n",
            "5  … Recent progress in natural language processi...  \n",
            "6  … The series consists of 50- to 150-page monog...  \n",
            "7  This paper describes AllenNLP, a platform for ...  \n",
            "8  … open-source Python natural language processi...  \n",
            "9  … a deep understanding of natural language by ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " I used ParseHub to scrape movie data from IMDb.\n",
        "\n",
        " Steps:\n",
        " 1. Download parse hub desktop app\n",
        " 2. Created a new project in ParseHub and pasted the IMDb URL.\n",
        " 3. Extracted movie names and URLs using the ParseHub selection tool.\n",
        " 4. Employed relative search to extract the release year of each movie.\n",
        " 5. Movie ratings extracted with relative search.\n",
        " 6. Carried out the data scraping to collect the needed information.\n",
        " 7. Saved the extracted data in CSV format.\n",
        " 8. The CSV file was transformed into PDF format. 8. PDF document was uploaded to UNT OneDrive.\n",
        "\n",
        " Link-- https://myunt-my.sharepoint.com/:b:/g/personal/swathinagilla_my_unt_edu/EcwnPnvwhh5Iho6e7a_F4X4BRFPyoud02G68szf_UveRWw?e=jzLDXh\n",
        "'''"
      ],
      "metadata": {
        "id": "07JlkjpaLuqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This assignment was relatively challenging and took much time to finish. I felt that web scraping was complex but enlightening.\n",
        "The most useful parts of the exercise were how to choose data for extraction or scraping and how to adjust to website structures.\n",
        "The Google Colab demo class file was pretty helpful; it gave the essential guidance and practical examples that supported my understanding\n",
        "of the web scraping process.\n",
        "\n",
        "A big challenge I had was with Question 4A, where the extraction process from social media accounts became complex. I am finding the\n",
        "first attempts really tricky but finally sought an alternative in ParseHub. While using ParseHub introduced its own complexities, I\n",
        "were able to surmount these by referring to YouTube tutorials, which facilitated my understanding of the tool's features.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}